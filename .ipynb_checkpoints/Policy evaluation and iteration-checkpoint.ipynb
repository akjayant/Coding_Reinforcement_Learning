{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Evaluation & Iteration given a policy on a GridWorld example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from gridworld import GridworldEnv\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = GridworldEnv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####     Args of environment:\n",
    "        policy: [S, A] shaped matrix representing the policy.\n",
    "        env: OpenAI env. env.P represents the transition probabilities (system probability) of the environment.\n",
    "            env.P[s][a] is a list of transition tuples (prob, next_state, reward, done).\n",
    "            env.nS is a number of states in the environment. \n",
    "            env.nA is a number of actions in the environment.\n",
    "        theta: We stop evaluation once our value function change is less than theta for all states.\n",
    "        discount_factor: Gamma discount factor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.nA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {0: [(1.0, 0, 0.0, True)],\n",
       "  1: [(1.0, 0, 0.0, True)],\n",
       "  2: [(1.0, 0, 0.0, True)],\n",
       "  3: [(1.0, 0, 0.0, True)]},\n",
       " 1: {0: [(1.0, 1, -1.0, False)],\n",
       "  1: [(1.0, 2, -1.0, False)],\n",
       "  2: [(1.0, 5, -1.0, False)],\n",
       "  3: [(1.0, 0, -1.0, True)]},\n",
       " 2: {0: [(1.0, 2, -1.0, False)],\n",
       "  1: [(1.0, 3, -1.0, False)],\n",
       "  2: [(1.0, 6, -1.0, False)],\n",
       "  3: [(1.0, 1, -1.0, False)]},\n",
       " 3: {0: [(1.0, 3, -1.0, False)],\n",
       "  1: [(1.0, 3, -1.0, False)],\n",
       "  2: [(1.0, 7, -1.0, False)],\n",
       "  3: [(1.0, 2, -1.0, False)]},\n",
       " 4: {0: [(1.0, 0, -1.0, True)],\n",
       "  1: [(1.0, 5, -1.0, False)],\n",
       "  2: [(1.0, 8, -1.0, False)],\n",
       "  3: [(1.0, 4, -1.0, False)]},\n",
       " 5: {0: [(1.0, 1, -1.0, False)],\n",
       "  1: [(1.0, 6, -1.0, False)],\n",
       "  2: [(1.0, 9, -1.0, False)],\n",
       "  3: [(1.0, 4, -1.0, False)]},\n",
       " 6: {0: [(1.0, 2, -1.0, False)],\n",
       "  1: [(1.0, 7, -1.0, False)],\n",
       "  2: [(1.0, 10, -1.0, False)],\n",
       "  3: [(1.0, 5, -1.0, False)]},\n",
       " 7: {0: [(1.0, 3, -1.0, False)],\n",
       "  1: [(1.0, 7, -1.0, False)],\n",
       "  2: [(1.0, 11, -1.0, False)],\n",
       "  3: [(1.0, 6, -1.0, False)]},\n",
       " 8: {0: [(1.0, 4, -1.0, False)],\n",
       "  1: [(1.0, 9, -1.0, False)],\n",
       "  2: [(1.0, 12, -1.0, False)],\n",
       "  3: [(1.0, 8, -1.0, False)]},\n",
       " 9: {0: [(1.0, 5, -1.0, False)],\n",
       "  1: [(1.0, 10, -1.0, False)],\n",
       "  2: [(1.0, 13, -1.0, False)],\n",
       "  3: [(1.0, 8, -1.0, False)]},\n",
       " 10: {0: [(1.0, 6, -1.0, False)],\n",
       "  1: [(1.0, 11, -1.0, False)],\n",
       "  2: [(1.0, 14, -1.0, False)],\n",
       "  3: [(1.0, 9, -1.0, False)]},\n",
       " 11: {0: [(1.0, 7, -1.0, False)],\n",
       "  1: [(1.0, 11, -1.0, False)],\n",
       "  2: [(1.0, 15, -1.0, True)],\n",
       "  3: [(1.0, 10, -1.0, False)]},\n",
       " 12: {0: [(1.0, 8, -1.0, False)],\n",
       "  1: [(1.0, 13, -1.0, False)],\n",
       "  2: [(1.0, 12, -1.0, False)],\n",
       "  3: [(1.0, 12, -1.0, False)]},\n",
       " 13: {0: [(1.0, 9, -1.0, False)],\n",
       "  1: [(1.0, 14, -1.0, False)],\n",
       "  2: [(1.0, 13, -1.0, False)],\n",
       "  3: [(1.0, 12, -1.0, False)]},\n",
       " 14: {0: [(1.0, 10, -1.0, False)],\n",
       "  1: [(1.0, 15, -1.0, True)],\n",
       "  2: [(1.0, 14, -1.0, False)],\n",
       "  3: [(1.0, 13, -1.0, False)]},\n",
       " 15: {0: [(1.0, 15, 0.0, True)],\n",
       "  1: [(1.0, 15, 0.0, True)],\n",
       "  2: [(1.0, 15, 0.0, True)],\n",
       "  3: [(1.0, 15, 0.0, True)]}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.P"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Evaluation (Sutton & Barto book) -\n",
    "\n",
    "<img src=\"./images/sutton_barto_policy_evaluation.png\">\n",
    "\n"
   ]
  },
  {
   "attachments": {
    "policy_eval_step.gif": {
     "image/gif": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementation - \n",
    "![policy_eval_step.gif](attachment:policy_eval_step.gif)\n",
    "<img src =\"./images/policy_eval_step.gif\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_eval(policy, env, discount_factor, theta):\n",
    "   \n",
    "    V = np.zeros(env.nS)\n",
    "    k=0\n",
    "    while True:\n",
    "        V_old = copy.deepcopy(V)\n",
    "        for state in range(env.nS):\n",
    "            agg_value = 0\n",
    "            for action in range(env.nA):\n",
    "                for prob_system,next_state,reward,done in env.P[state][action]:\n",
    "                    agg_value += (V[next_state]*discount_factor + reward)*prob_system*policy[state][action]\n",
    "            V[state] = agg_value\n",
    "        change = np.linalg.norm(np.abs(V - V_old))\n",
    "        if change<theta:\n",
    "            break\n",
    "    return np.array(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "given_policy = np.ones([env.nS, env.nA]) / env.nA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 16 states, 4 actions - 16*4 matrix\n",
    "given_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0.        , -13.99997737, -19.99996746, -21.99996418,\n",
       "       -13.99997737, -17.99997222, -19.99996984, -19.99997019,\n",
       "       -19.99996746, -19.99996984, -17.99997455, -13.999981  ,\n",
       "       -21.99996418, -19.99997019, -13.999981  ,   0.        ])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy_eval(given_policy,env,1,0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from dennybritz repo\n",
    "expected_v = np.array([0, -14, -20, -22, -14, -18, -20, -20, -20, -20, -18, -14, -22, -20, -14, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Iteration (Sutton & Barto book)\n",
    "\n",
    "<img src=\"./images/policy_iteration.png\">\n",
    "\n",
    "\n",
    "    1)Randomly initialize a policy\n",
    "    2)Evaluate the policy\n",
    "    3)Select the best action according to current policy (present_best_action in code)\n",
    "    4)Evaluate value of each action by doing one-step lookahed (action_expected_value_vector in code) as - \n",
    "\n",
    "   <img src=\"./images/action_value.gif\">\n",
    "  ### .\n",
    "  \n",
    "    5)Select the best action from one step lookahed action_expected_value_vector -  improved_best_action\n",
    "    6)Change the policy\n",
    "    7)if present_best_action=improved_best_action exit(); else Go to step 2\n",
    "\n",
    "*NOTE - terminating condition I have used in policy evaluation: L2-norm(|V_new - V_old|)< theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def action_value_vector(env,present_state,V,discount_factor):\n",
    "    action_expected_value_vector = np.zeros(env.nA)\n",
    "    for action in range(env.nA):\n",
    "        for prob_system,next_state,reward,done in env.P[present_state][action]:\n",
    "            action_expected_value_vector[action] += prob_system*(reward + discount_factor*V[next_state])\n",
    "    return action_expected_value_vector\n",
    "\n",
    "\n",
    "def policy_iter(env):\n",
    "    #random initialization of policy \n",
    "    policy = np.ones([env.nS, env.nA]) / env.nA\n",
    "    theta = 0.001\n",
    "    discount_factor = 1\n",
    "    while True:\n",
    "        #evaluate policy\n",
    "        V = policy_eval(policy,env,discount_factor,theta)\n",
    "        #print(V)\n",
    "        policy_change = False\n",
    "        for state in range(env.nS):\n",
    "            present_best_action = np.argmax(policy[state])\n",
    "            #one step lookahead to calculate expected value of actions at current policy and its value function V\n",
    "            action_values = action_value_vector(env,state,V,discount_factor)\n",
    "            #select the improved best action\n",
    "            improved_best_action = np.argmax(action_values)\n",
    "            #CHANGE POLICY\n",
    "            action_switch = [0,0,0,0]\n",
    "            action_switch[improved_best_action] = 1\n",
    "            action_switch = np.array(action_switch)\n",
    "            policy[state] = action_switch\n",
    "            #flag for optimality\n",
    "            if present_best_action != improved_best_action:\n",
    "                policy_change=True\n",
    "        if policy_change==False:\n",
    "            return policy,V\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[1., 0., 0., 0.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [1., 0., 0., 0.]]),\n",
       " array([ 0., -1., -2., -3., -1., -2., -3., -2., -2., -3., -2., -1., -3.,\n",
       "        -2., -1.,  0.]))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy_iter(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0, -1, -2, -3, -1, -2, -3, -2, -2, -3, -2, -1, -3, -2, -1,  0])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#from dennybritz repo\n",
    "expected_v = np.array([ 0, -1, -2, -3, -1, -2, -3, -2, -2, -3, -2, -1, -3, -2, -1,  0])\n",
    "expected_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
